{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART1 demo\n",
    "\n",
    "Adaptive Resonance Theory Neural Networks\n",
    "by Aman Ahuja | github.com/amanahuja | twitter: @amanqa\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Reminders: \n",
    "\n",
    " * ART1 accepts binary inputs only. \n",
    " *  \n",
    "\n",
    "In this example:\n",
    " * We'll use 10x10 ASCII blocks to demonstrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Load data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array([\"   O \",\n",
    "        \"  O O\",\n",
    "        \"    O\",\n",
    "        \"  O O\",\n",
    "        \"    O\",\n",
    "        \"  O O\",\n",
    "        \"    O\",\n",
    "        \" OO O\",\n",
    "        \" OO  \",\n",
    "        \" OO O\",\n",
    "        \" OO  \",\n",
    "        \"OOO  \",\n",
    "        \"OO   \",\n",
    "        \"O    \",\n",
    "        \"OO   \",\n",
    "        \"OOO  \",\n",
    "        \"OOOO \",\n",
    "        \"OOOOO\",\n",
    "        \"O    \",\n",
    "        \" O   \",\n",
    "        \"  O  \",\n",
    "        \"   O \",\n",
    "        \"    O\",\n",
    "        \"  O O\",\n",
    "        \" OO O\",\n",
    "        \" OO  \",\n",
    "        \"OOO  \",\n",
    "        \"OO   \",\n",
    "        \"OOOO \",\n",
    "        \"OOOOO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simplied ART1\n",
    "\n",
    "class ART1:\n",
    "    \"\"\"\n",
    "    ART class\n",
    "    modified Aman Ahuja\n",
    "    \n",
    "    Usage example:\n",
    "    --------------\n",
    "    # Create a ART network with input of size 5 and 20 internal units\n",
    "    >>> network = ART(5,10,0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=5, m=10, rho=.5):\n",
    "        '''\n",
    "        Create network with specified shape\n",
    "        \n",
    "        For Input array I of size n, we need n input nodes in F1. \n",
    "        \n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            feature dimension of input; number of nodes in F1\n",
    "        m : int\n",
    "            Number of neurons in F2 competition layer\n",
    "            max number of categories\n",
    "            compare to n_class\n",
    "        rho : float\n",
    "            Vigilance parameter\n",
    "            larger rho: less inclusive prototypes\n",
    "            smaller rho: more generalization\n",
    "        \n",
    "        internal paramters\n",
    "        ---------- \n",
    "        F1: array of size (n)\n",
    "            array of F1 neurons\n",
    "        F2: array of size (m)\n",
    "            array of F2 neurons\n",
    "        Wf: array of shape (m x n)\n",
    "            Feed-Forward weights\n",
    "            These are Tk\n",
    "        Wb: array of shape (n x m)\n",
    "            Feed-back weights\n",
    "        n_cats : int\n",
    "            Number of F2 neurons that are active\n",
    "            (at any given time, number of category templates)\n",
    "        \n",
    "        '''\n",
    "        # Comparison layer\n",
    "        self.F1 = np.ones(n)\n",
    "        \n",
    "        # Recognition layer\n",
    "        self.F2 = np.ones(m)\n",
    "        \n",
    "        # Feed-forward weights\n",
    "        self.Wf = np.random.random((m,n))\n",
    "        \n",
    "        # Feed-back weights\n",
    "        self.Wb = np.random.random((n,m))\n",
    "        \n",
    "        # Vigilance parameter\n",
    "        self.rho = rho\n",
    "        \n",
    "        # Number of active units in F2\n",
    "        self.n_cats = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset whole network to start conditions\n",
    "        \"\"\"\n",
    "        self.F1 = np.ones(n)\n",
    "        self.F2 = np.ones(m)\n",
    "        self.Wf = np.random.random((m,n))\n",
    "        self.Wb = np.random.random((n,m))\n",
    "        self.n_cats = 0 \n",
    "        \n",
    "    def learn(self, X):\n",
    "        \"\"\"Learn X\n",
    "        use i as index over inputs or F1\n",
    "        use k as index over categories or F2\n",
    "        \"\"\" \n",
    "\n",
    "        # Compute F2 output using feed forward weights\n",
    "        self.F2[...] = np.dot(self.Wf, X)\n",
    "        \n",
    "        # collect and sort the output of each active node (C)\n",
    "        C = np.argsort(self.F2[:self.n_cats].ravel())[::-1]\n",
    "\n",
    "        for k in C:\n",
    "            # compute nearest memory\n",
    "            d = (self.Wb[:,k]*X).sum()/X.sum()\n",
    "\n",
    "            # Check if d is above the vigilance level\n",
    "            if d >= self.rho:\n",
    "                ww = self._learn_data(k, X)\n",
    "                return ww\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        # No match found within vigilance level\n",
    "        # If there's room, increase the number of active units\n",
    "        # and make the newly active unit to learn data\n",
    "        if self.n_cats < self.F2.size:\n",
    "            k = self.n_cats  # index of last category\n",
    "            ww = self._learn_data(k, X)\n",
    "            self.n_cats += 1\n",
    "            return ww\n",
    "        else: \n",
    "            return None,None\n",
    "\n",
    "    def _learn_data(self, node, dat):\n",
    "        \"\"\"\n",
    "        node : i : F2 node\n",
    "        dat  : X : input data\n",
    "        \"\"\" \n",
    "        self._validate_data(dat)\n",
    "        \n",
    "        # Learn data\n",
    "        self.Wb[:,node] *= dat\n",
    "        self.Wf[node,:] = self.Wb[:,node]/(0.5+self.Wb[:,node].sum())\n",
    "        return self.Wb[:,node], node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        C = np.dot(self.Wf[:self.n_cats], X)\n",
    "\n",
    "        #return active F2 node, unless none are active\n",
    "        if np.all(C == 0):\n",
    "            return None\n",
    "\n",
    "        return np.argmax(C)\n",
    "\n",
    "    def _validate_data(self, dat):\n",
    "        \"\"\"\n",
    "        dat is a single input record\n",
    "        Checks: data must be 1s and 0s\n",
    "        \"\"\"\n",
    "        pass_checks = True\n",
    "        \n",
    "        # Dimensions must match\n",
    "        if dat.shape[0] != len(self.F1):\n",
    "            pass_checks = False\n",
    "            msg = \"Input dimensins mismatch.\"\n",
    "        \n",
    "        # Data must be 1s or 0s\n",
    "        if not np.all((dat == 1) | (dat == 0)):\n",
    "            pass_checks = False\n",
    "            msg = \"Input must be binary.\"\n",
    "        \n",
    "        if pass_checks:\n",
    "            return True\n",
    "        else: \n",
    "            raise Exception(\"Data does not validate: {}\".format(msg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_data(data): \n",
    "    \"\"\"\n",
    "    Convert to numpy array\n",
    "    Convert to 1s and 0s\n",
    "    \n",
    "    \"\"\"\n",
    "    # Look at first row\n",
    "    if data[0]: \n",
    "        irow = data[0]\n",
    "\n",
    "        # get size\n",
    "        idat_size = len(irow)\n",
    "\n",
    "        # get unique characters\n",
    "        chars = False\n",
    "        while not chars: \n",
    "            chars = get_unique_chars(irow, reverse=True)\n",
    "        char1, char2 = chars\n",
    "\n",
    "    outdata = []\n",
    "    idat = np.zeros(idat_size, dtype=bool)\n",
    "\n",
    "    for irow in data:\n",
    "        idat = [x==char1 for x in irow]\n",
    "        outdata.append(idat)\n",
    "    \n",
    "    return np.array(outdata).astype(int)\n",
    "\n",
    "def get_unique_chars(irow, reverse=False):\n",
    "    \"\"\"\n",
    "    Get unique characters in data\n",
    "    Helper function\n",
    "    ---- \n",
    "    reverse:   bool\n",
    "        Reverses order of the two chars returned\n",
    "    \"\"\"\n",
    "    chars = Counter(irow)\n",
    "    if len(chars) > 2: \n",
    "        raise Exception(\"Data is not binary\")\n",
    "    elif len(chars) < 2: \n",
    "        # first row doesn't contain both chars\n",
    "        return False, False\n",
    "\n",
    "    # Reorder here?\n",
    "    if reverse: \n",
    "        char2, char1 = chars.keys()\n",
    "    else: \n",
    "        char1, char2 = chars.keys()\n",
    "    \n",
    "    return char1, char2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n categories:  7\n"
     ]
    }
   ],
   "source": [
    "network = ART1(n=5, m=7, rho=0.5)\n",
    "\n",
    "# preprocess data\n",
    "data_cleaned = preprocess_data(data)\n",
    "\n",
    "# learn data array, row by row\n",
    "for row in data_cleaned:\n",
    "    network.learn(row)\n",
    "\n",
    "print \"n categories: \", network.n_cats\n",
    "#print tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predictions\n",
    "\n",
    "Let's see the clusters created through training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.n_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----\n",
      "   O \n",
      "   O \n",
      "\n",
      "2\n",
      "-----\n",
      "    O\n",
      "    O\n",
      "    O\n",
      "    O\n",
      "\n",
      "4\n",
      "-----\n",
      "  O O\n",
      "  O O\n",
      "  O O\n",
      " OO O\n",
      " OO O\n",
      "OOOOO\n",
      "  O  \n",
      "  O O\n",
      " OO O\n",
      "OOOOO\n",
      "\n",
      "5\n",
      "-----\n",
      " OO  \n",
      " OO  \n",
      " O   \n",
      " OO  \n",
      "\n",
      "6\n",
      "-----\n",
      "OOO  \n",
      "OO   \n",
      "O    \n",
      "OO   \n",
      "OOO  \n",
      "OOOO \n",
      "O    \n",
      "OOO  \n",
      "OO   \n",
      "OOOO \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output_dict = defaultdict(list)\n",
    "\n",
    "for row, row_cleaned in zip (data, data_cleaned): \n",
    "    pred = network.predict(row_cleaned)\n",
    "    output_dict[pred].append(row)\n",
    "\n",
    "for k,v in output_dict.iteritems():\n",
    "    print k\n",
    "    print '-----'\n",
    "    for row in v: \n",
    "        print row\n",
    "    print \n",
    "#   \\  print \"'{}':{}\".format(\n",
    "#         row, \n",
    "#         network.predict(row_cleaned))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the weights as patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern #0\n",
      "[0 0 0 1 0]\n",
      "\n",
      "Pattern #1\n",
      "[0 0 1 0 1]\n",
      "\n",
      "Pattern #2\n",
      "[0 0 0 0 1]\n",
      "\n",
      "Pattern #3\n",
      "[0 0 1 0 1]\n",
      "\n",
      "Pattern #4\n",
      "[0 0 1 0 1]\n",
      "\n",
      "Pattern #5\n",
      "[0 1 0 0 0]\n",
      "\n",
      "Pattern #6\n",
      "[1 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_units = network.Wf[:network.n_cats]\n",
    "for idx, CU_weights in enumerate(cluster_units):\n",
    "    pattern = CU_weights\n",
    "    pattern = pattern.astype(bool)\n",
    "    \n",
    "    print \"Pattern #{}\".format(idx)\n",
    "    print pattern.astype(int)\n",
    "    print\n",
    "    #preprocess_data(pattern)\n",
    "    #print np.round(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
