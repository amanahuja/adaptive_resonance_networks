{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART1 demo\n",
    "\n",
    "Adaptive Resonance Theory Neural Networks\n",
    "by Aman Ahuja | github.com/amanahuja | twitter: @amanqa\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Reminders: \n",
    "\n",
    " * ART1 accepts binary inputs only. \n",
    " *  \n",
    "\n",
    "In this example:\n",
    " * We'll use 10x10 ASCII blocks to demonstrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Load data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure we're in the root directory\n",
    "\n",
    "pwd = os.getcwd()\n",
    "if pwd.endswith('ipynb'):\n",
    "    os.chdir('..')\n",
    "    \n",
    "#print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = 'data'\n",
    "print os.listdir(data_dir)\n",
    "\n",
    "# ASCII data file\n",
    "data_file = 'ASCII_01.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, data_file), 'r') as f: \n",
    "    raw_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out raw_data to see what it looks like\n",
    "# print raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data into a usable form here\n",
    "data = [d.strip() for d in raw_data.split('\\n\\n')]\n",
    "data = [d for d in data if d is not '']\n",
    "\n",
    "data = [d.replace('\\n', '') for d in data]\n",
    "\n",
    "# print the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cleaning and proceprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(data): \n",
    "    \"\"\"\n",
    "    Convert to numpy array\n",
    "    Convert to 1s and 0s\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get useful information from first row\n",
    "    if data[0]: \n",
    "        irow = data[0]\n",
    "\n",
    "        # get size\n",
    "        idat_size = len(irow)\n",
    "\n",
    "        # get unique characters\n",
    "        chars = False\n",
    "        while not chars: \n",
    "            chars = get_unique_chars(irow, reverse=True)\n",
    "        char1, char2 = chars\n",
    "\n",
    "    outdata = []\n",
    "    idat = np.zeros(idat_size, dtype=bool)\n",
    "\n",
    "    #convert to boolean using the chars identified\n",
    "    for irow in data:\n",
    "        assert len(irow) == idat_size, \"data row lengths not consistent\"\n",
    "        idat = [x==char1 for x in irow]\n",
    "        # note: idat is a list of bools\n",
    "        idat =list(np.array(idat).astype(int))\n",
    "        outdata.append(idat)\n",
    "    \n",
    "    outdata = np.array(outdata)\n",
    "    return outdata.astype(int)\n",
    "\n",
    "def get_unique_chars(irow, reverse=False):\n",
    "    \"\"\"\n",
    "    Get unique characters in data\n",
    "    Helper function\n",
    "    ---- \n",
    "    reverse:   bool\n",
    "        Reverses order of the two chars returned\n",
    "    \"\"\"\n",
    "    chars = Counter(irow)\n",
    "    if len(chars) > 2: \n",
    "        raise Exception(\"Data is not binary\")\n",
    "    elif len(chars) < 2: \n",
    "        # first row doesn't contain both chars\n",
    "        return False, False\n",
    "\n",
    "    # Reorder here?\n",
    "    if reverse: \n",
    "        char2, char1 = chars.keys()\n",
    "    else: \n",
    "        char1, char2 = chars.keys()\n",
    "    \n",
    "    return char1, char2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "data_cleaned = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_ASCII(raw):\n",
    "    out = \"{}\\n{}\\n{}\\n{}\\n{}\".format(\n",
    "        raw[:5],\n",
    "        raw[5:10],\n",
    "        raw[10:15],\n",
    "        raw[15:20],\n",
    "        raw[20:25],\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simplied ART1\n",
    "\n",
    "class ART1:\n",
    "    \"\"\"\n",
    "    ART class\n",
    "    modified Aman Ahuja\n",
    "    \n",
    "    Usage example:\n",
    "    --------------\n",
    "    # Create a ART network with input of size 5 and 20 internal units\n",
    "    >>> network = ART(5,10,0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=5, m=10, rho=.5):\n",
    "        '''\n",
    "        Create network with specified shape\n",
    "        \n",
    "        For Input array I of size n, we need n input nodes in F1. \n",
    "        \n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            feature dimension of input; number of nodes in F1\n",
    "        m : int\n",
    "            Number of neurons in F2 competition layer\n",
    "            max number of categories\n",
    "            compare to n_class\n",
    "        rho : float\n",
    "            Vigilance parameter\n",
    "            larger rho: less inclusive prototypes\n",
    "            smaller rho: more generalization\n",
    "        \n",
    "        internal paramters\n",
    "        ---------- \n",
    "        F1: array of size (n)\n",
    "            array of F1 neurons\n",
    "        F2: array of size (m)\n",
    "            array of F2 neurons\n",
    "        Wf: array of shape (m x n)\n",
    "            Feed-Forward weights\n",
    "            These are Tk\n",
    "        Wb: array of shape (n x m)\n",
    "            Feed-back weights\n",
    "        n_cats : int\n",
    "            Number of F2 neurons that are active\n",
    "            (at any given time, number of category templates)\n",
    "        \n",
    "        '''\n",
    "        # Comparison layer\n",
    "        self.F1 = np.ones(n)\n",
    "        \n",
    "        # Recognition layer\n",
    "        self.F2 = np.ones(m)\n",
    "        \n",
    "        # Feed-forward weights\n",
    "        self.Wf = np.random.random((m,n))\n",
    "        \n",
    "        # Feed-back weights\n",
    "        self.Wb = np.random.random((n,m))\n",
    "        \n",
    "        # Vigilance parameter\n",
    "        self.rho = rho\n",
    "        \n",
    "        # Number of active units in F2\n",
    "        self.n_cats = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset whole network to start conditions\n",
    "        \"\"\"\n",
    "        self.F1 = np.ones(n)\n",
    "        self.F2 = np.ones(m)\n",
    "        self.Wf = np.random.random((m,n))\n",
    "        self.Wb = np.random.random((n,m))\n",
    "        self.n_cats = 0 \n",
    "        \n",
    "    def learn(self, X):\n",
    "        \"\"\"Learn X\n",
    "        use i as index over inputs or F1\n",
    "        use k as index over categories or F2\n",
    "        \"\"\" \n",
    "\n",
    "        # Compute F2 output using feed forward weights\n",
    "        self.F2[...] = np.dot(self.Wf, X)\n",
    "        \n",
    "        # collect and sort the output of each active node (C)\n",
    "        C = np.argsort(self.F2[:self.n_cats].ravel())[::-1]\n",
    "\n",
    "        for k in C:\n",
    "            # compute nearest memory\n",
    "            d = (self.Wb[:,k]*X).sum()/X.sum()\n",
    "\n",
    "            # Check if d is above the vigilance level\n",
    "            if d >= self.rho:\n",
    "                ww = self._learn_data(k, X)\n",
    "                return ww\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        # No match found within vigilance level\n",
    "        # If there's room, increase the number of active units\n",
    "        # and make the newly active unit to learn data\n",
    "        if self.n_cats < self.F2.size:\n",
    "            k = self.n_cats  # index of last category\n",
    "            ww = self._learn_data(k, X)\n",
    "            self.n_cats += 1\n",
    "            return ww\n",
    "        else: \n",
    "            return None,None\n",
    "\n",
    "    def _learn_data(self, node, dat):\n",
    "        \"\"\"\n",
    "        node : i : F2 node\n",
    "        dat  : X : input data\n",
    "        \"\"\" \n",
    "        self._validate_data(dat)\n",
    "        \n",
    "        # Learn data\n",
    "        self.Wb[:,node] *= dat\n",
    "        self.Wf[node,:] = self.Wb[:,node]/(0.5+self.Wb[:,node].sum())\n",
    "        return self.Wb[:,node], node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        C = np.dot(self.Wf[:self.n_cats], X)\n",
    "\n",
    "        #return active F2 node, unless none are active\n",
    "        if np.all(C == 0):\n",
    "            return None\n",
    "\n",
    "        return np.argmax(C)\n",
    "\n",
    "    def _validate_data(self, dat):\n",
    "        \"\"\"\n",
    "        dat is a single input record\n",
    "        Checks: data must be 1s and 0s\n",
    "        \"\"\"\n",
    "        pass_checks = True\n",
    "        \n",
    "        # Dimensions must match\n",
    "        if dat.shape[0] != len(self.F1):\n",
    "            pass_checks = False\n",
    "            msg = \"Input dimensins mismatch.\"\n",
    "        \n",
    "        # Data must be 1s or 0s\n",
    "        if not np.all((dat == 1) | (dat == 0)):\n",
    "            pass_checks = False\n",
    "            msg = \"Input must be binary.\"\n",
    "        \n",
    "        if pass_checks:\n",
    "            return True\n",
    "        else: \n",
    "            raise Exception(\"Data does not validate: {}\".format(msg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# create network\n",
    "\n",
    "input_row_size = 25\n",
    "max_categories = 10\n",
    "rho = 0.20\n",
    "\n",
    "network = ART1(n=input_row_size, m=max_categories, rho=rho)\n",
    "\n",
    "# preprocess data\n",
    "data_cleaned = preprocess_data(data)\n",
    "\n",
    "# shuffle data? \n",
    "np.random.seed(1221)\n",
    "np.random.shuffle(data_cleaned)\n",
    "\n",
    "# learn data array, row by row\n",
    "for row in data_cleaned:\n",
    "    network.learn(row)\n",
    "\n",
    "print\n",
    "print \"n rows of data:         \", len(data_cleaned)\n",
    "print \"max categories allowed: \", max_categories\n",
    "print \"rho:                    \", rho\n",
    "\n",
    "print \"n categories used:      \", network.n_cats\n",
    "print\n",
    "\n",
    "\n",
    "# output results, row by row\n",
    "output_dict = defaultdict(list)\n",
    "\n",
    "for row, row_cleaned in zip (data, data_cleaned): \n",
    "    pred = network.predict(row_cleaned)\n",
    "    output_dict[pred].append(row)\n",
    "\n",
    "for k,v in output_dict.iteritems():\n",
    "    print \"category: {}, ({} members)\".format(k, len(v))\n",
    "    print '-'*20\n",
    "    for row in v: \n",
    "        print display_ASCII(row)\n",
    "        print\n",
    "    print \n",
    "#   \\  print \"'{}':{}\".format(\n",
    "#         row, \n",
    "#         network.predict(row_cleaned))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
